{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "import weave\n",
    "from configparser import ConfigParser\n",
    "import json\n",
    "from pathlib import Path\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_parser = ConfigParser()\n",
    "config_parser.read(\"config.cfg\")\n",
    "LLAMA_KEY = config_parser.get(\"DEFAULT\", \"LLAMA_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path(\"data/centerEmbed/ce1.json\").open(encoding=\"UTF-8\") as source:\n",
    "     objects = json.load(source)\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"id\": i,\n",
    "        \"context\": ex[\"Context\"],\n",
    "        \"question\": ex[\"Q\"],\n",
    "        \"target\": ex[\"A\"],\n",
    "        \"level\": ex[\"level\"],\n",
    "    }\n",
    "for i, ex in enumerate(objects)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama(weave.Model):\n",
    "\n",
    "    model_name : str\n",
    "    prompt_template : str\n",
    "\n",
    "    @property\n",
    "    def api_key(self):\n",
    "        return LLAMA_KEY\n",
    "\n",
    "    @property\n",
    "    def api(self):\n",
    "        return AsyncOpenAI(\n",
    "            api_key=self.api_key, \n",
    "            base_url=\"https://api.llama-api.com\"\n",
    "        )\n",
    "    \n",
    "\n",
    "    def format(self, context : str, question : str, params : dict, **kwargs) -> dict:\n",
    "\n",
    "        prompt = self.prompt_template.format(context=context, question=question)\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            **params,\n",
    "            **kwargs\n",
    "        }\n",
    "\n",
    "    @weave.op()\n",
    "    async def predict(\n",
    "        self,\n",
    "        context : str,\n",
    "        question : str,\n",
    "        params : dict = {},\n",
    "        **kwargs\n",
    "    ):\n",
    "        with weave.attributes(kwargs):\n",
    "\n",
    "            payload = self.format(context, question, params)\n",
    "\n",
    "            response = await self.api.chat.completions.create(\n",
    "                model=self.model_name, \n",
    "                **payload\n",
    "            )\n",
    "            if response is None:\n",
    "                raise ValueError(\"No response from model\")\n",
    "\n",
    "            result = response.choices[0].message.content\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"You will be given an example consisting of a context and a question to answer. The answer should always be of this form \"The N V the N\", where N stands for a single word that is a noun, and V stands for a single word that is a verb. \n",
    "Here are two samples:\n",
    "\n",
    "        \"Context\": \"The student the man noticed seemed happy\",\n",
    "        \"Question\": \"Who saw who?\",\n",
    "        \"Answer\": \"The man saw the student.\",\n",
    "\n",
    "\n",
    "        \"Context\": \"The teacher the student saw hit is dead\",\n",
    "        \"Question\": \"Who saw who?\",\n",
    "        \"Answer\": \"The student saw the teacher.\",\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Now answer the question:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Llama(\n",
    "    name=\"llama-7b-chat\",\n",
    "    description=\"Weave model for Llama\",\n",
    "    model_name=\"llama-7b-chat\",\n",
    "    prompt_template=PROMPT_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "import editdistance\n",
    "\n",
    "from utils.loggers import logger\n",
    "\n",
    "\n",
    "def postprocess(string):\n",
    "    return string.strip().lower().strip(\"answer:\").strip()\n",
    "\n",
    "@weave.op()\n",
    "def evaluator(target: str, model_output: str) -> dict:\n",
    "    \"\"\" Evaluate the model output against the target. We\n",
    "    consider the model output correct if it exactly matches\n",
    "    the target or if the edit distance between the two\n",
    "    strings is less than 2.\"\"\"\n",
    "\n",
    "    model_output = postprocess(model_output)\n",
    "    edit_distance = editdistance.eval(target, model_output)\n",
    "    \n",
    "    exact_match = target == model_output\n",
    "    fuzzy_match = edit_distance < 2\n",
    "    correct =  exact_match | fuzzy_match\n",
    "\n",
    "    logger.info(f\"{target=} | {model_output=} | {correct=}\")\n",
    "    return {\n",
    "        'correct': correct,\n",
    "        'edit_distance': edit_distance,\n",
    "        'exact_match': exact_match,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# SAMPLE EXAMPLES\n",
    "################\n",
    "\n",
    "sample: list = random.sample(examples, 10)\n",
    "\n",
    "################\n",
    "# RUN EVALUATION\n",
    "################\n",
    "\n",
    "weave.init(f\"llama-7b-chat-CE1-n{len(sample)}-test3\")\n",
    "\n",
    "\n",
    "\n",
    "evaluation = weave.Evaluation(\n",
    "    dataset=sample,\n",
    "    scorers=[evaluator],\n",
    "    trials=1,\n",
    "\n",
    ")\n",
    "\n",
    "output = await evaluation.evaluate(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
